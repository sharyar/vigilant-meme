{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranging-tyler",
   "metadata": {},
   "source": [
    "# Task 2 - Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-whole",
   "metadata": {},
   "source": [
    "For task 2, we were tasked in the creation of a predictor using three different models/approaches, which are shown in the following order: (1) Regression, (2) Support Vector Machine and (3) Random Forest. \n",
    "\n",
    "The analyzed dataset for this task was the Ionosphere dataset, which describes radar data that is collector by a system that is located in Goose Bay Labrador. The system, consisting of a phased array of 16 high frequency antennas, targeted free electrons in the ionosphere. From these observations, a \"good\" radar return and \"bad\" radar return can be recorded, where a \"good\" return is indicative that the radar return showed evidence of some kind of structure in the ionophere and a \"bad\" return indicates that the signal passed through the ionosphere.\n",
    "\n",
    "Hence, for classification, our goal is the creation of a predictor that should perform the following classification:\n",
    "g for good and b for bad = function(ionosphere features)\n",
    "\n",
    "In our analysis of each created model, each model is conducted and analyzed using a split of training and testing data. Following the intial construction of all models, a 10-fold cross validation is performed to compare model performance betewen different models/approaches. Lastly, this information, along with the results of a t-test are used to identify the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-small",
   "metadata": {},
   "source": [
    "### Ionosphere Data Set Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-operations",
   "metadata": {},
   "source": [
    "Prior to model creation, the ionosphere data was pre-processed. Note that upon visual inspection of the data, the second column was found to have no variance (e.g. all values were the same) and therefore does not provide a significant contribution to the data. Hence, the second column was removed from the analysis.\n",
    "\n",
    "For classification, the label was changed to a binary encoding where \"g\" was mapped to a 1 value and \"b\" was mapped to a 0 value. This is necessary to construct a logistic regression model so that the regression model maps to a logical value following the performed classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinate-ordering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83508</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.73739</td>\n",
       "      <td>-0.14706</td>\n",
       "      <td>0.84349</td>\n",
       "      <td>-0.05567</td>\n",
       "      <td>0.90441</td>\n",
       "      <td>-0.04622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04202</td>\n",
       "      <td>0.83479</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12815</td>\n",
       "      <td>0.86660</td>\n",
       "      <td>-0.10714</td>\n",
       "      <td>0.90546</td>\n",
       "      <td>-0.04307</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95113</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.95183</td>\n",
       "      <td>-0.02723</td>\n",
       "      <td>0.93438</td>\n",
       "      <td>-0.01920</td>\n",
       "      <td>0.94590</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01361</td>\n",
       "      <td>0.93522</td>\n",
       "      <td>0.04925</td>\n",
       "      <td>0.93159</td>\n",
       "      <td>0.08168</td>\n",
       "      <td>0.94066</td>\n",
       "      <td>-0.00035</td>\n",
       "      <td>0.91483</td>\n",
       "      <td>0.04712</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>-0.00034</td>\n",
       "      <td>0.93207</td>\n",
       "      <td>-0.03227</td>\n",
       "      <td>0.95177</td>\n",
       "      <td>-0.03431</td>\n",
       "      <td>0.95584</td>\n",
       "      <td>0.02446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>0.92489</td>\n",
       "      <td>0.02542</td>\n",
       "      <td>0.92120</td>\n",
       "      <td>0.02242</td>\n",
       "      <td>0.92459</td>\n",
       "      <td>0.00442</td>\n",
       "      <td>0.92697</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90608</td>\n",
       "      <td>-0.01657</td>\n",
       "      <td>0.98122</td>\n",
       "      <td>-0.01989</td>\n",
       "      <td>0.95691</td>\n",
       "      <td>-0.03646</td>\n",
       "      <td>0.85746</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02099</td>\n",
       "      <td>0.89147</td>\n",
       "      <td>-0.07760</td>\n",
       "      <td>0.82983</td>\n",
       "      <td>-0.17238</td>\n",
       "      <td>0.96022</td>\n",
       "      <td>-0.03757</td>\n",
       "      <td>0.87403</td>\n",
       "      <td>-0.16243</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84710</td>\n",
       "      <td>0.13533</td>\n",
       "      <td>0.73638</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>0.87873</td>\n",
       "      <td>0.08260</td>\n",
       "      <td>0.88928</td>\n",
       "      <td>-0.09139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15114</td>\n",
       "      <td>0.81147</td>\n",
       "      <td>-0.04822</td>\n",
       "      <td>0.78207</td>\n",
       "      <td>-0.00703</td>\n",
       "      <td>0.75747</td>\n",
       "      <td>-0.06678</td>\n",
       "      <td>0.85764</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  f2       f3       f4       f5       f6       f7       f8       f9  \\\n",
       "0     1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1     1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2     1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3     1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4     1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "..   ..  ..      ...      ...      ...      ...      ...      ...      ...   \n",
       "346   1   0  0.83508  0.08298  0.73739 -0.14706  0.84349 -0.05567  0.90441   \n",
       "347   1   0  0.95113  0.00419  0.95183 -0.02723  0.93438 -0.01920  0.94590   \n",
       "348   1   0  0.94701 -0.00034  0.93207 -0.03227  0.95177 -0.03431  0.95584   \n",
       "349   1   0  0.90608 -0.01657  0.98122 -0.01989  0.95691 -0.03646  0.85746   \n",
       "350   1   0  0.84710  0.13533  0.73638 -0.06151  0.87873  0.08260  0.88928   \n",
       "\n",
       "         f10  ...      f26      f27      f28      f29      f30      f31  \\\n",
       "0    0.03760  ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267   \n",
       "1   -0.04549  ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626   \n",
       "2    0.01198  ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436   \n",
       "3    0.00000  ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682   \n",
       "4   -0.16399  ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707   \n",
       "..       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "346 -0.04622  ... -0.04202  0.83479  0.00123  1.00000  0.12815  0.86660   \n",
       "347  0.01606  ...  0.01361  0.93522  0.04925  0.93159  0.08168  0.94066   \n",
       "348  0.02446  ...  0.03193  0.92489  0.02542  0.92120  0.02242  0.92459   \n",
       "349  0.00110  ... -0.02099  0.89147 -0.07760  0.82983 -0.17238  0.96022   \n",
       "350 -0.09139  ... -0.15114  0.81147 -0.04822  0.78207 -0.00703  0.75747   \n",
       "\n",
       "         f32      f33      f34  label  \n",
       "0   -0.54487  0.18641 -0.45300      g  \n",
       "1   -0.06288 -0.13738 -0.02447      b  \n",
       "2   -0.24180  0.56045 -0.38238      g  \n",
       "3    1.00000 -0.32382  1.00000      b  \n",
       "4   -0.59573 -0.04608 -0.65697      g  \n",
       "..       ...      ...      ...    ...  \n",
       "346 -0.10714  0.90546 -0.04307      g  \n",
       "347 -0.00035  0.91483  0.04712      g  \n",
       "348  0.00442  0.92697 -0.00577      g  \n",
       "349 -0.03757  0.87403 -0.16243      g  \n",
       "350 -0.06678  0.85764 -0.06151      g  \n",
       "\n",
       "[351 rows x 35 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Naming of all columns\n",
    "colnames=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', \n",
    "          'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29',\n",
    "          'f30', 'f31', 'f32', 'f33', 'f34', 'label']\n",
    "\n",
    "ionosphere_df = pd.read_csv('data_files/ionosphere.data', names=colnames, header=None)\n",
    "ionosphere_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "herbal-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categories as 1 and 0 (g = 1, b = 0)\n",
    "ionosphere_df['label'] = ionosphere_df.label.astype('category')\n",
    "encoding = {'g': 1, 'b': 0}\n",
    "ionosphere_df.label.replace(encoding, inplace=True)\n",
    "\n",
    "# Removal of the second column (f2) as all of its values are identical and there is no variance\n",
    "ionosphere_df.drop(columns=['f2'], inplace=True)\n",
    "\n",
    "X = ionosphere_df.values[:, :-1]\n",
    "y = ionosphere_df.values[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outstanding-custody",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Perform scaling on feature set data\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-discussion",
   "metadata": {},
   "source": [
    "<b>(1) Logistic Regression Model</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-boating",
   "metadata": {},
   "source": [
    "The logistic regression model was evaluated for scaled data, and selection of 25 features using the SelectKBest method. Following the creation of the model, a 10 fold cross validation was conducted to determine the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dried-climb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Initialize cross validation for k-fold = 10\n",
    "cross_validation = KFold(n_splits=10, random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cosmetic-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "logreg_model = linear_model.LogisticRegression(solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "looking-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 1.        , 0.94285714, 0.85714286, 0.85714286,\n",
       "       0.88571429, 0.91428571, 0.91428571, 0.82857143, 0.88571429])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled data\n",
    "logreg_scores = cross_val_score(logreg_model, X_scaled, y, scoring='accuracy', cv=cross_validation, n_jobs=-1)\n",
    "logreg_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-rates",
   "metadata": {},
   "source": [
    "<b>(2) Support Vector Machine</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-infrared",
   "metadata": {},
   "source": [
    "Blurb about Support Vector Machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "recreational-ordering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_soft_model = SVC(kernel='linear')\n",
    "svc_soft_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "activated-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yp_soft = svc_soft_model.predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interpreted-damage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Margin Model accuracy: 0.9430199430199431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Soft Margin Model accuracy:\",accuracy_score(y,Yp_soft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "going-tunisia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83333333, 0.97142857, 0.94285714, 0.94285714, 0.8       ,\n",
       "       0.88571429, 0.88571429, 0.91428571, 0.8       , 0.91428571])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-fold cross validation\n",
    "svc_soft_model_scores = cross_val_score(svc_soft_model, X_scaled, y, scoring='accuracy', cv=cross_validation, n_jobs=-1)\n",
    "svc_soft_model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-grade",
   "metadata": {},
   "source": [
    "<b>(3) Random Forest</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-mobile",
   "metadata": {},
   "source": [
    "Blurb about Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amber-founder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=12, n_estimators=50, random_state=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "random_forest_model = RandomForestClassifier(n_estimators=50, random_state=0, max_depth=12)\n",
    "random_forest_model.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "opposed-hartford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91666667, 0.94285714, 0.94285714, 0.94285714, 0.88571429,\n",
       "       0.94285714, 0.94285714, 0.97142857, 0.88571429, 0.94285714])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K-fold cross validation\n",
    "random_forest_model_scores = cross_val_score(random_forest_model, X_scaled, y, scoring='accuracy', cv=cross_validation, n_jobs=-1)\n",
    "random_forest_model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-colors",
   "metadata": {},
   "source": [
    "<b>Model Performance Comparison</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-nicaragua",
   "metadata": {},
   "source": [
    "Blurb about Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-test, comparison of model performance\n",
    "# Placeholder for printing out all results across all models\n",
    "import pandas as pd\n",
    "pd.DataFrame({'':['Widthout normalization','With normalization','With standarization','scikit-learn'],\n",
    "              'Train score':[score(X_train, Y_train, w),\n",
    "                             score(X_train_norm, Y_train, w_n),\n",
    "                             score(X_train_std, Y_train, w_s),\n",
    "                             sklreg.score(X_train,Y_train)],\n",
    "              'Test score':[score(X_test, Y_test, w),\n",
    "                            score(normalization(X_test), Y_test, w_n),\n",
    "                            score(standardization(X_test), Y_test, w_s),\n",
    "                            sklreg.score(X_test,Y_test)]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
