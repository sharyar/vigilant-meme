{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "involved-snowboard",
   "metadata": {},
   "source": [
    "# Task 2 - Ionosphere Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-saying",
   "metadata": {},
   "source": [
    "Sharyar Memon, 1299819\n",
    "<br> \n",
    "Alyson Wu, 1399985"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-olympus",
   "metadata": {},
   "source": [
    "For task 2, we were tasked in the creation of a predictor using three different models/approaches, which are shown in the following order: (1) Regression, (2) Support Vector Machine and (3) Random Forest. \n",
    "\n",
    "The analyzed dataset for this task was the Ionosphere dataset, which describes radar data that is collector by a system that is located in Goose Bay Labrador. The system, consisting of a phased array of 16 high frequency antennas, targeted free electrons in the ionosphere. From these observations, a \"good\" radar return and \"bad\" radar return can be recorded, where a \"good\" return is indicative that the radar return showed evidence of some kind of structure in the ionophere and a \"bad\" return indicates that the signal passed through the ionosphere.\n",
    "\n",
    "Hence, for classification, our goal is the creation of a predictor that should perform the following classification:\n",
    "g for good and b for bad = function(ionosphere features)\n",
    "\n",
    "In our analysis of each created model, a 10-fold cross validation is performed to compare model performance betewen different models/approaches. Lastly, this information, along with the results of a ANOVA are used to identify the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-treatment",
   "metadata": {},
   "source": [
    "## Ionosphere Data Set Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legislative-humor",
   "metadata": {},
   "source": [
    "Prior to model creation, the ionosphere data was pre-processed. Note that upon visual inspection of the data, the second column was found to have no variance (e.g. all values were the same) and therefore does not provide a significant contribution to the data. Hence, the second column was removed from the analysis.\n",
    "\n",
    "For classification, the label was changed to a binary encoding where \"g\" was mapped to a 1 value and \"b\" was mapped to a 0 value. This is necessary to construct a logistic regression model so that the regression model maps to a logical value following the performed classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expanded-dressing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f26</th>\n",
       "      <th>f27</th>\n",
       "      <th>f28</th>\n",
       "      <th>f29</th>\n",
       "      <th>f30</th>\n",
       "      <th>f31</th>\n",
       "      <th>f32</th>\n",
       "      <th>f33</th>\n",
       "      <th>f34</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99539</td>\n",
       "      <td>-0.05889</td>\n",
       "      <td>0.85243</td>\n",
       "      <td>0.02306</td>\n",
       "      <td>0.83398</td>\n",
       "      <td>-0.37708</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.03760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.51171</td>\n",
       "      <td>0.41078</td>\n",
       "      <td>-0.46168</td>\n",
       "      <td>0.21266</td>\n",
       "      <td>-0.34090</td>\n",
       "      <td>0.42267</td>\n",
       "      <td>-0.54487</td>\n",
       "      <td>0.18641</td>\n",
       "      <td>-0.45300</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.18829</td>\n",
       "      <td>0.93035</td>\n",
       "      <td>-0.36156</td>\n",
       "      <td>-0.10868</td>\n",
       "      <td>-0.93597</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.04549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.26569</td>\n",
       "      <td>-0.20468</td>\n",
       "      <td>-0.18401</td>\n",
       "      <td>-0.19040</td>\n",
       "      <td>-0.11593</td>\n",
       "      <td>-0.16626</td>\n",
       "      <td>-0.06288</td>\n",
       "      <td>-0.13738</td>\n",
       "      <td>-0.02447</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.03365</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.00485</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.12062</td>\n",
       "      <td>0.88965</td>\n",
       "      <td>0.01198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.40220</td>\n",
       "      <td>0.58984</td>\n",
       "      <td>-0.22145</td>\n",
       "      <td>0.43100</td>\n",
       "      <td>-0.17365</td>\n",
       "      <td>0.60436</td>\n",
       "      <td>-0.24180</td>\n",
       "      <td>0.56045</td>\n",
       "      <td>-0.38238</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.45161</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.71216</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.90695</td>\n",
       "      <td>0.51613</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.20099</td>\n",
       "      <td>0.25682</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.32382</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.02401</td>\n",
       "      <td>0.94140</td>\n",
       "      <td>0.06531</td>\n",
       "      <td>0.92106</td>\n",
       "      <td>-0.23255</td>\n",
       "      <td>0.77152</td>\n",
       "      <td>-0.16399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.65158</td>\n",
       "      <td>0.13290</td>\n",
       "      <td>-0.53206</td>\n",
       "      <td>0.02431</td>\n",
       "      <td>-0.62197</td>\n",
       "      <td>-0.05707</td>\n",
       "      <td>-0.59573</td>\n",
       "      <td>-0.04608</td>\n",
       "      <td>-0.65697</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83508</td>\n",
       "      <td>0.08298</td>\n",
       "      <td>0.73739</td>\n",
       "      <td>-0.14706</td>\n",
       "      <td>0.84349</td>\n",
       "      <td>-0.05567</td>\n",
       "      <td>0.90441</td>\n",
       "      <td>-0.04622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04202</td>\n",
       "      <td>0.83479</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.12815</td>\n",
       "      <td>0.86660</td>\n",
       "      <td>-0.10714</td>\n",
       "      <td>0.90546</td>\n",
       "      <td>-0.04307</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95113</td>\n",
       "      <td>0.00419</td>\n",
       "      <td>0.95183</td>\n",
       "      <td>-0.02723</td>\n",
       "      <td>0.93438</td>\n",
       "      <td>-0.01920</td>\n",
       "      <td>0.94590</td>\n",
       "      <td>0.01606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01361</td>\n",
       "      <td>0.93522</td>\n",
       "      <td>0.04925</td>\n",
       "      <td>0.93159</td>\n",
       "      <td>0.08168</td>\n",
       "      <td>0.94066</td>\n",
       "      <td>-0.00035</td>\n",
       "      <td>0.91483</td>\n",
       "      <td>0.04712</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.94701</td>\n",
       "      <td>-0.00034</td>\n",
       "      <td>0.93207</td>\n",
       "      <td>-0.03227</td>\n",
       "      <td>0.95177</td>\n",
       "      <td>-0.03431</td>\n",
       "      <td>0.95584</td>\n",
       "      <td>0.02446</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>0.92489</td>\n",
       "      <td>0.02542</td>\n",
       "      <td>0.92120</td>\n",
       "      <td>0.02242</td>\n",
       "      <td>0.92459</td>\n",
       "      <td>0.00442</td>\n",
       "      <td>0.92697</td>\n",
       "      <td>-0.00577</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.90608</td>\n",
       "      <td>-0.01657</td>\n",
       "      <td>0.98122</td>\n",
       "      <td>-0.01989</td>\n",
       "      <td>0.95691</td>\n",
       "      <td>-0.03646</td>\n",
       "      <td>0.85746</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.02099</td>\n",
       "      <td>0.89147</td>\n",
       "      <td>-0.07760</td>\n",
       "      <td>0.82983</td>\n",
       "      <td>-0.17238</td>\n",
       "      <td>0.96022</td>\n",
       "      <td>-0.03757</td>\n",
       "      <td>0.87403</td>\n",
       "      <td>-0.16243</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.84710</td>\n",
       "      <td>0.13533</td>\n",
       "      <td>0.73638</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>0.87873</td>\n",
       "      <td>0.08260</td>\n",
       "      <td>0.88928</td>\n",
       "      <td>-0.09139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.15114</td>\n",
       "      <td>0.81147</td>\n",
       "      <td>-0.04822</td>\n",
       "      <td>0.78207</td>\n",
       "      <td>-0.00703</td>\n",
       "      <td>0.75747</td>\n",
       "      <td>-0.06678</td>\n",
       "      <td>0.85764</td>\n",
       "      <td>-0.06151</td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1  f2       f3       f4       f5       f6       f7       f8       f9  \\\n",
       "0     1   0  0.99539 -0.05889  0.85243  0.02306  0.83398 -0.37708  1.00000   \n",
       "1     1   0  1.00000 -0.18829  0.93035 -0.36156 -0.10868 -0.93597  1.00000   \n",
       "2     1   0  1.00000 -0.03365  1.00000  0.00485  1.00000 -0.12062  0.88965   \n",
       "3     1   0  1.00000 -0.45161  1.00000  1.00000  0.71216 -1.00000  0.00000   \n",
       "4     1   0  1.00000 -0.02401  0.94140  0.06531  0.92106 -0.23255  0.77152   \n",
       "..   ..  ..      ...      ...      ...      ...      ...      ...      ...   \n",
       "346   1   0  0.83508  0.08298  0.73739 -0.14706  0.84349 -0.05567  0.90441   \n",
       "347   1   0  0.95113  0.00419  0.95183 -0.02723  0.93438 -0.01920  0.94590   \n",
       "348   1   0  0.94701 -0.00034  0.93207 -0.03227  0.95177 -0.03431  0.95584   \n",
       "349   1   0  0.90608 -0.01657  0.98122 -0.01989  0.95691 -0.03646  0.85746   \n",
       "350   1   0  0.84710  0.13533  0.73638 -0.06151  0.87873  0.08260  0.88928   \n",
       "\n",
       "         f10  ...      f26      f27      f28      f29      f30      f31  \\\n",
       "0    0.03760  ... -0.51171  0.41078 -0.46168  0.21266 -0.34090  0.42267   \n",
       "1   -0.04549  ... -0.26569 -0.20468 -0.18401 -0.19040 -0.11593 -0.16626   \n",
       "2    0.01198  ... -0.40220  0.58984 -0.22145  0.43100 -0.17365  0.60436   \n",
       "3    0.00000  ...  0.90695  0.51613  1.00000  1.00000 -0.20099  0.25682   \n",
       "4   -0.16399  ... -0.65158  0.13290 -0.53206  0.02431 -0.62197 -0.05707   \n",
       "..       ...  ...      ...      ...      ...      ...      ...      ...   \n",
       "346 -0.04622  ... -0.04202  0.83479  0.00123  1.00000  0.12815  0.86660   \n",
       "347  0.01606  ...  0.01361  0.93522  0.04925  0.93159  0.08168  0.94066   \n",
       "348  0.02446  ...  0.03193  0.92489  0.02542  0.92120  0.02242  0.92459   \n",
       "349  0.00110  ... -0.02099  0.89147 -0.07760  0.82983 -0.17238  0.96022   \n",
       "350 -0.09139  ... -0.15114  0.81147 -0.04822  0.78207 -0.00703  0.75747   \n",
       "\n",
       "         f32      f33      f34  label  \n",
       "0   -0.54487  0.18641 -0.45300      g  \n",
       "1   -0.06288 -0.13738 -0.02447      b  \n",
       "2   -0.24180  0.56045 -0.38238      g  \n",
       "3    1.00000 -0.32382  1.00000      b  \n",
       "4   -0.59573 -0.04608 -0.65697      g  \n",
       "..       ...      ...      ...    ...  \n",
       "346 -0.10714  0.90546 -0.04307      g  \n",
       "347 -0.00035  0.91483  0.04712      g  \n",
       "348  0.00442  0.92697 -0.00577      g  \n",
       "349 -0.03757  0.87403 -0.16243      g  \n",
       "350 -0.06678  0.85764 -0.06151      g  \n",
       "\n",
       "[351 rows x 35 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Naming of all columns\n",
    "colnames=['f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', \n",
    "          'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29',\n",
    "          'f30', 'f31', 'f32', 'f33', 'f34', 'label']\n",
    "\n",
    "ionosphere_df = pd.read_csv('data_files/ionosphere.data', names=colnames, header=None)\n",
    "ionosphere_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "operating-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the categories as 1 and 0 (g = 1, b = 0)\n",
    "ionosphere_df['label'] = ionosphere_df.label.astype('category')\n",
    "encoding = {'g': 1, 'b': 0}\n",
    "ionosphere_df.label.replace(encoding, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-garage",
   "metadata": {},
   "source": [
    "Upon closer inspection of the data set, the second column (feature 2) was removed as all of it's values were identical and therefore due to the lack of variance the feature was discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rotary-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of the second column (f2) as all of its values are identical and there is no variance\n",
    "ionosphere_df.drop(columns=['f2'], inplace=True)\n",
    "\n",
    "X = ionosphere_df.values[:, :-1]\n",
    "y = ionosphere_df.values[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-debut",
   "metadata": {},
   "source": [
    "For a more complete analysis, the data set was standardized and normalized to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "varying-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(Features):\n",
    "    \"\"\"\n",
    "    Take a set of features return normalize values using\n",
    "    (x -xmin) / (xmax – xmin) of each feature\n",
    "    \"\"\"\n",
    "    fmin = np.min(Features,axis=0)\n",
    "    fmax = np.max(Features,axis=0)\n",
    "    Features_norm = (Features - fmin)/(fmax-fmin)\n",
    "    return Features_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "green-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Perform standardization on feature set data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Perform normalization of the feature set data\n",
    "X_normalized = normalization(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-double",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-brook",
   "metadata": {},
   "source": [
    "The logistic regression model was evaluated for scaled and normalized data. Unlike the previously conducted linear regression model (performed for task 1), the logistic regression is a classification method that will assign observations to a discrete set of classes (instead of a continuous values as was seen using the linear regression model). Following the creation of the model, a 10 fold cross validation was conducted to determine the model performance. The definition of a 10 fold cross validaiton is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prostate-camel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "def kfold_10_cross_validation(model, X, Y):\n",
    "    # Initialize cross validation for k-fold = 10\n",
    "    cross_validation = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    \n",
    "    return cross_val_score(model, X, Y, scoring='accuracy', cv=cross_validation, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-castle",
   "metadata": {},
   "source": [
    "**Train 3 different models with no scaling/normalization, scaling, normalization**\n",
    "\n",
    "For this exercise, we created three different models using different variations of the original data set (original data, scaled data and normalized data). Following this, the performance of each model was compared, and lastly, feature importance was \n",
    "1. Create logistic regression models\n",
    "2. Compare performance\n",
    "3. Select the best logistic regression model\n",
    "\n",
    "From this analysis we aimed to retrieve the model that performed the best of the three. Sklearn's native logistic regression model was leveraged for the creation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "julian-dominican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular Scores</th>\n",
       "      <th>Scaled Scores</th>\n",
       "      <th>Normalized Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Regular Scores  Scaled Scores  Normalized Scores\n",
       "0        0.805556       0.833333           0.805556\n",
       "1        0.942857       1.000000           0.971429\n",
       "2        1.000000       0.942857           1.000000\n",
       "3        0.857143       0.857143           0.885714\n",
       "4        0.828571       0.857143           0.800000\n",
       "5        0.828571       0.885714           0.857143\n",
       "6        0.914286       0.914286           0.885714\n",
       "7        0.857143       0.914286           0.885714\n",
       "8        0.800000       0.828571           0.828571\n",
       "9        0.885714       0.885714           0.942857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# Utilize sklearn's native logistic regression model\n",
    "logreg_model = linear_model.LogisticRegression(solver='lbfgs')\n",
    "\n",
    "logreg_scores = kfold_10_cross_validation(logreg_model, X, y)\n",
    "logreg_scaled_scores = kfold_10_cross_validation(logreg_model, X_scaled, y)\n",
    "logreg_normalized_scores = kfold_10_cross_validation(logreg_model, X_normalized, y)\n",
    "\n",
    "scores = {\n",
    "    'Regular Scores': logreg_scores,\n",
    "    'Scaled Scores': logreg_scaled_scores,\n",
    "    'Normalized Scores': logreg_normalized_scores,\n",
    "}\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affecting-territory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular</th>\n",
       "      <th>Scaled</th>\n",
       "      <th>Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.871984</td>\n",
       "      <td>0.891905</td>\n",
       "      <td>0.886270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.060986</td>\n",
       "      <td>0.050217</td>\n",
       "      <td>0.064439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Regular    Scaled  Normalized\n",
       "Mean Score Value    0.871984  0.891905    0.886270\n",
       "Standard Deviation  0.060986  0.050217    0.064439"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the mean value of the scores to find best performing model\n",
    "logreg_stats_values = {\n",
    "    'Regular': [logreg_scores.mean(), logreg_scores.std()],\n",
    "    'Scaled': [logreg_scaled_scores.mean(), logreg_scaled_scores.std()],\n",
    "    'Normalized': [logreg_normalized_scores.mean(), logreg_normalized_scores.std()],  \n",
    "}\n",
    "\n",
    "pd.DataFrame(logreg_stats_values, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-drunk",
   "metadata": {},
   "source": [
    "From the mean values of the 10 fold cross validation using the regular data, scaled data and normalized data, the logistic model with the highest mean score value was selected. In this case, the logistic model performed best with the scaled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-blanket",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-geography",
   "metadata": {},
   "source": [
    "A support vector machine (SVM) model was also evaluated for regular, scaled and normalized data, as well as different kernel functions (linear, poly and rbf). From these generated models, a k fold cross validation was performed and analyzed to determine the best model.\n",
    "\n",
    "**Train 3 different models with no scaling/normalization, scaling, normalization**\n",
    "\n",
    "For this exercise, we created three different models using different variations of the original data set (original data, scaled data and normalized data). Following this, the performance of each model was compared, and lastly, feature importance was \n",
    "1. Create support vector machine (SVM) models using different variantions of the original data set (original, scaled, normalized) and using different kernel functions (linear, polynomial, radial basis function)\n",
    "2. Compare performance\n",
    "3. Select the best support vector machine (SVM) model\n",
    "\n",
    "From this analysis we aimed to retrieve the model that performed the best of the three. Sklearn's native SVC model was leveraged for the creation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-physiology",
   "metadata": {},
   "source": [
    "**SVM with Linear Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "different-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_linear_model = SVC(kernel='linear').fit(X, y)\n",
    "svc_linear_model_scaled = SVC(kernel='linear').fit(X_scaled, y)\n",
    "svc_linear_model_normalized = SVC(kernel='linear').fit(X_normalized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "front-murder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "svc_linear_model_scores = kfold_10_cross_validation(svc_linear_model, X, y)\n",
    "svc_linear_model_scaled_scores = kfold_10_cross_validation(svc_linear_model_scaled, X_scaled, y)\n",
    "svc_linear_model_normalized_scores = kfold_10_cross_validation(svc_linear_model_normalized, X_normalized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "equipped-macedonia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular Scores</th>\n",
       "      <th>Scaled Scores</th>\n",
       "      <th>Normalized Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Regular Scores  Scaled Scores  Normalized Scores\n",
       "0        0.805556       0.833333           0.833333\n",
       "1        0.942857       0.971429           0.942857\n",
       "2        0.971429       0.942857           1.000000\n",
       "3        0.857143       0.942857           0.885714\n",
       "4        0.828571       0.800000           0.828571\n",
       "5        0.857143       0.885714           0.885714\n",
       "6        0.885714       0.885714           0.885714\n",
       "7        0.885714       0.914286           0.885714\n",
       "8        0.800000       0.800000           0.800000\n",
       "9        0.885714       0.914286           0.914286"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {\n",
    "    'Regular Scores': svc_linear_model_scores,\n",
    "    'Scaled Scores': svc_linear_model_scaled_scores,\n",
    "    'Normalized Scores': svc_linear_model_normalized_scores,\n",
    "}\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "consistent-waters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular</th>\n",
       "      <th>Scaled</th>\n",
       "      <th>Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.871984</td>\n",
       "      <td>0.889048</td>\n",
       "      <td>0.886190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.052343</td>\n",
       "      <td>0.057303</td>\n",
       "      <td>0.055224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Regular    Scaled  Normalized\n",
       "Mean Score Value    0.871984  0.889048    0.886190\n",
       "Standard Deviation  0.052343  0.057303    0.055224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the mean value of the scores to find best performing model\n",
    "svc_linear_mean_values = {\n",
    "    'Regular': [svc_linear_model_scores.mean(), svc_linear_model_scores.std()],\n",
    "    'Scaled': [svc_linear_model_scaled_scores.mean(), svc_linear_model_scaled_scores.std()],\n",
    "    'Normalized': [svc_linear_model_normalized_scores.mean(), svc_linear_model_normalized_scores.std()],  \n",
    "}\n",
    "\n",
    "pd.DataFrame(svc_linear_mean_values, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-japan",
   "metadata": {},
   "source": [
    "The best performing SVM model (using the linear kernel) was found to be using scaled data. Note that the mean score value across models were comparable, with the normalized data performing similarly to the model that was tested and trained using scaled data. The same analysis is performed using the polynomial kernel and rbf kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-ghana",
   "metadata": {},
   "source": [
    "**SVM with Polynomial Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "essential-amendment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular</th>\n",
       "      <th>Scaled</th>\n",
       "      <th>Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.903016</td>\n",
       "      <td>0.758175</td>\n",
       "      <td>0.911825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.057540</td>\n",
       "      <td>0.058111</td>\n",
       "      <td>0.051420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Regular    Scaled  Normalized\n",
       "Mean Score Value    0.903016  0.758175    0.911825\n",
       "Standard Deviation  0.057540  0.058111    0.051420"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate SVM\n",
    "svc_poly_model = SVC(kernel='poly').fit(X, y)\n",
    "svc_poly_model_scaled = SVC(kernel='poly').fit(X_scaled, y)\n",
    "svc_poly_model_normalized = SVC(kernel='poly').fit(X_normalized, y)\n",
    "\n",
    "# K-fold cross validation\n",
    "svc_poly_model_scores = kfold_10_cross_validation(svc_poly_model, X, y)\n",
    "svc_poly_model_scaled_scores = kfold_10_cross_validation(svc_poly_model_scaled, X_scaled, y)\n",
    "svc_poly_model_normalized_scores = kfold_10_cross_validation(svc_poly_model_normalized, X_normalized, y)\n",
    "\n",
    "# Print the mean value of the scores to find best performing model\n",
    "svc_poly_mean_values = {\n",
    "    'Regular': [svc_poly_model_scores.mean(), svc_poly_model_scores.std()],\n",
    "    'Scaled': [svc_poly_model_scaled_scores.mean(), svc_poly_model_scaled_scores.std()],\n",
    "    'Normalized': [svc_poly_model_normalized_scores.mean(), svc_poly_model_normalized_scores.std()],  \n",
    "}\n",
    "\n",
    "pd.DataFrame(svc_poly_mean_values, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-escape",
   "metadata": {},
   "source": [
    "The best performing SVM model (using the polynomial kernel) was found to be using normalized data. Note that the model performed significantly worse with scaled data and had comparative performance between using the regular data and normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-saudi",
   "metadata": {},
   "source": [
    "**SVM with Radial Basis Kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "disabled-convertible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular</th>\n",
       "      <th>Scaled</th>\n",
       "      <th>Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.934683</td>\n",
       "      <td>0.934683</td>\n",
       "      <td>0.934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.039762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Regular    Scaled  Normalized\n",
       "Mean Score Value    0.934683  0.934683    0.934683\n",
       "Standard Deviation  0.039762  0.039762    0.039762"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate SVM\n",
    "svc_rbf_model = SVC(kernel='rbf').fit(X, y)\n",
    "svc_rbf_model_scaled = SVC(kernel='rbf').fit(X_scaled, y)\n",
    "svc_rbf_model_normalized = SVC(kernel='rbf').fit(X_normalized, y)\n",
    "\n",
    "# K-fold cross validation\n",
    "svc_rbf_model_scores = kfold_10_cross_validation(svc_rbf_model, X, y)\n",
    "svc_rbf_model_scaled_scores = kfold_10_cross_validation(svc_rbf_model_scaled, X_scaled, y)\n",
    "svc_rbf_model_normalized_scores = kfold_10_cross_validation(svc_rbf_model_normalized, X_normalized, y)\n",
    "\n",
    "# Print the mean value of the scores to find best performing model\n",
    "svc_rbf_mean_values = {\n",
    "    'Regular': [svc_rbf_model_scores.mean(), svc_rbf_model_scores.std()],\n",
    "    'Scaled': [svc_rbf_model_scaled_scores.mean(), svc_rbf_model_scaled_scores.std()],\n",
    "    'Normalized': [svc_rbf_model_normalized_scores.mean(), svc_rbf_model_normalized_scores.std()],  \n",
    "}\n",
    "\n",
    "pd.DataFrame(svc_rbf_mean_values, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-mercury",
   "metadata": {},
   "source": [
    "The same mean score values and standard deviation values were found across all generated SVM models using the rbf kernel. Therefore any of the three can be chosen for the subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-vehicle",
   "metadata": {},
   "source": [
    "**Comparison Across SVM Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-decrease",
   "metadata": {},
   "source": [
    "Comparison across SVM models was achieved by looking at the mean score value (taken from the array generated following k-fold cross validation) and the standard deviation of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "powered-tonight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Linear</th>\n",
       "      <th>Best Polynomial</th>\n",
       "      <th>Best RBF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.889048</td>\n",
       "      <td>0.911825</td>\n",
       "      <td>0.934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.057303</td>\n",
       "      <td>0.051420</td>\n",
       "      <td>0.039762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Best Linear  Best Polynomial  Best RBF\n",
       "Mean Score Value       0.889048         0.911825  0.934683\n",
       "Standard Deviation     0.057303         0.051420  0.039762"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_stats_scores = {\n",
    "    'Best Linear': svc_linear_mean_values['Scaled'],\n",
    "    'Best Polynomial': svc_poly_mean_values['Normalized'],\n",
    "    'Best RBF': svc_rbf_mean_values['Regular'],\n",
    "}\n",
    "\n",
    "pd.DataFrame(svm_stats_scores, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-proof",
   "metadata": {},
   "source": [
    "Following the comparison of all 3 SVM models, the SVM model generated using the radial basis function was found to have the highest mean score value and hence was chosen as the best SVM model for further comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superior-niagara",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-switzerland",
   "metadata": {},
   "source": [
    "Lastly, a random forest model was evaluated for different number of estimators (25, 50, and 100). Application of the regular data, standardized data and normalized data were also applied to find the best performing model. First, a random forest model consisting of 50 estimators was analyzed:  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "discrete-tobago",
   "metadata": {},
   "source": [
    "**Train Random Forest Models**\n",
    "\n",
    "For this exercise, we created  models using different variations of the original data set (original data, scaled data and normalized data) and different numbers of estimators (25, 50 and 100).\n",
    "1. Create random forest models using orignal, scaled and normalized data with 25, 50 and 100 estimators\n",
    "2. Compare performance\n",
    "3. Select the best random forest model\n",
    "\n",
    "From this analysis we aimed to retrieve the model that performed the best of the three. Sklearn's native RandomForestClassifier was leveraged for the creation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "impressive-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Creation of a random forest model with 50 estimators\n",
    "random_forest_model = RandomForestClassifier(n_estimators=50, random_state=0, max_depth=12)\n",
    "random_forest = random_forest_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "played-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform k-fold validation using regular data, scaled data and normalized data\n",
    "random_forest_scores = kfold_10_cross_validation(random_forest, X, y)\n",
    "random_forest_scaled_scores = kfold_10_cross_validation(random_forest, X_scaled, y)\n",
    "random_forest_normalized_scores = kfold_10_cross_validation(random_forest, X_normalized, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "functional-disposition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Regular</th>\n",
       "      <th>Scaled</th>\n",
       "      <th>Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.931667</td>\n",
       "      <td>0.931667</td>\n",
       "      <td>0.931667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.026040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Regular    Scaled  Normalized\n",
       "Mean Score Value    0.931667  0.931667    0.931667\n",
       "Standard Deviation  0.026040  0.026040    0.026040"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the mean value of the scores to find best performing model\n",
    "random_forest_mean_values = {\n",
    "    'Regular': [random_forest_scores.mean(), random_forest_scores.std()],\n",
    "    'Scaled': [random_forest_scaled_scores.mean(), random_forest_scaled_scores.std()],\n",
    "    'Normalized': [random_forest_normalized_scores.mean(), random_forest_normalized_scores.std()],  \n",
    "}\n",
    "\n",
    "pd.DataFrame(random_forest_mean_values, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-ensemble",
   "metadata": {},
   "source": [
    "Following the anlaysis, it is clear that the mean score value and standard deviation does not seem to be impacted by the data being used (the regular data, standardized data and the normalized data all have the same values). Therefore we should evaluate based on the number of predictors. For this, 25 predictors and 100 predictors were also evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dress-title",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model_100 = RandomForestClassifier(n_estimators=100, random_state=0, max_depth=12).fit(X, y)\n",
    "random_forest_100_scores = kfold_10_cross_validation(random_forest_model_100, X, y)\n",
    "\n",
    "random_forest_model_25 = RandomForestClassifier(n_estimators=25, random_state=0, max_depth=12).fit(X, y)\n",
    "random_forest_25_scores = kfold_10_cross_validation(random_forest_model_25, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-processor",
   "metadata": {},
   "source": [
    "**Overall Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-advertiser",
   "metadata": {},
   "source": [
    "Comparison across Random Forest models was achieved by looking at the mean score value (taken from the array generated following k-fold cross validation) and the standard deviation of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "million-appointment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>25 Estimators Scores</th>\n",
       "      <th>50 Estimators Scores</th>\n",
       "      <th>100 Estimators Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.942857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.971429</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.971429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   25 Estimators Scores  50 Estimators Scores  100 Estimators Scores\n",
       "0              0.916667              0.916667               0.916667\n",
       "1              0.942857              0.942857               0.942857\n",
       "2              0.942857              0.942857               0.971429\n",
       "3              0.942857              0.942857               0.942857\n",
       "4              0.914286              0.885714               0.914286\n",
       "5              0.942857              0.942857               0.942857\n",
       "6              0.942857              0.942857               0.942857\n",
       "7              0.942857              0.971429               0.942857\n",
       "8              0.885714              0.885714               0.857143\n",
       "9              0.971429              0.942857               0.971429"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {\n",
    "    '25 Estimators Scores': random_forest_25_scores,\n",
    "    '50 Estimators Scores': random_forest_scores,\n",
    "    '100 Estimators Scores':random_forest_100_scores,\n",
    "}\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acceptable-traveler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>25 Estimators</th>\n",
       "      <th>50 Estimators</th>\n",
       "      <th>100 Estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.934524</td>\n",
       "      <td>0.931667</td>\n",
       "      <td>0.934524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.022112</td>\n",
       "      <td>0.026040</td>\n",
       "      <td>0.031285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    25 Estimators  50 Estimators  100 Estimators\n",
       "Mean Score Value         0.934524       0.931667        0.934524\n",
       "Standard Deviation       0.022112       0.026040        0.031285"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the mean value of the scores to find best performing model\n",
    "random_forest_mean_values = {\n",
    "    '25 Estimators': [random_forest_25_scores.mean(), random_forest_25_scores.std()],\n",
    "    '50 Estimators': [random_forest_scores.mean(), random_forest_scores.std()],\n",
    "    '100 Estimators': [random_forest_100_scores.mean(), random_forest_100_scores.std()],  \n",
    "}\n",
    "\n",
    "pd.DataFrame(random_forest_mean_values, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-guatemala",
   "metadata": {},
   "source": [
    "For the selection of the best random forest model, a high mean score value and a high standard deviation was desired. Therfore, the random forest model with 100 estimators was selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-ceiling",
   "metadata": {},
   "source": [
    "### Model Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-packet",
   "metadata": {},
   "source": [
    "Comparison of the generated models were achieved through the average of the model score (the output of the 10 k-fold cross validation), and through the ANOVA test, or the Analysis of Variance test. Note that when conducting the ANOVA test, models were compared in pairs and also compared as a whole group.\n",
    "\n",
    "\n",
    "**ANOVA Test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-dutch",
   "metadata": {},
   "source": [
    "Using the scipy module f_oneway method, models could directly be compared using the ANOVA test. When using this method, models are compared to see whether they lie in the same distribution (following the failure to reject the null hypothesis), or within different distributions (following the rejection of the null hypothesis). When the probability value fell below the alpha value of 0.05, the null hypothesis was rejected, signifying at a 95% confidence that the two models formed two separate distributions. Otherwise we failed to reject the null hypothesis and the models are of the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "coordinated-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of variance test\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Definition of the ANOVA test\n",
    "def ANOVA_test(list_of_models):\n",
    "    stat, p = f_oneway(*list_of_models)\n",
    "    \n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        result = 'Same distributions (fail to reject H0)'\n",
    "    else:\n",
    "        result = 'Different distributions (reject H0)'\n",
    "    return [stat, p, result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passing-malaysia",
   "metadata": {},
   "source": [
    "Comparing pairs, it was found that the SVC model was of the same distribution as both the random forest model and the logistic regression model. The logistic regression model was found to be of different distributions than the random forest model. When comparing across all models, they were all found to be of different distributions. This is indicative that while there may be overlap with the SVC model, it was not significant in the comparison of all three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "animated-detroit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stat Value</th>\n",
       "      <th>P value</th>\n",
       "      <th>Comparison Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SVC vs. Random Forest</th>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.992594</td>\n",
       "      <td>Same distributions (fail to reject H0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression vs. SVC</th>\n",
       "      <td>4.014218</td>\n",
       "      <td>0.060407</td>\n",
       "      <td>Same distributions (fail to reject H0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression vs. Random Forest</th>\n",
       "      <td>4.670019</td>\n",
       "      <td>0.044418</td>\n",
       "      <td>Different distributions (reject H0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All Models</th>\n",
       "      <td>3.229066</td>\n",
       "      <td>0.055289</td>\n",
       "      <td>Same distributions (fail to reject H0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Stat Value   P value  \\\n",
       "SVC vs. Random Forest                   0.000089  0.992594   \n",
       "Logistic Regression vs. SVC             4.014218  0.060407   \n",
       "Logistic Regression vs. Random Forest   4.670019  0.044418   \n",
       "All Models                              3.229066  0.055289   \n",
       "\n",
       "                                                            Comparison Result  \n",
       "SVC vs. Random Forest                  Same distributions (fail to reject H0)  \n",
       "Logistic Regression vs. SVC            Same distributions (fail to reject H0)  \n",
       "Logistic Regression vs. Random Forest     Different distributions (reject H0)  \n",
       "All Models                             Same distributions (fail to reject H0)  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare svc soft model and random forest model\n",
    "svc_vs_forest = ANOVA_test([svc_rbf_model_scores, random_forest_100_scores])\n",
    "logreg_vs_svc = ANOVA_test([logreg_scaled_scores, svc_rbf_model_scores])\n",
    "logreg_vs_forest = ANOVA_test([logreg_scaled_scores, random_forest_100_scores])\n",
    "all_model_compare = ANOVA_test([logreg_scaled_scores, random_forest_100_scores, svc_rbf_model_scores])\n",
    "\n",
    "# Print the mean value of the scores to find best performing model\n",
    "comparison_values = {\n",
    "    'SVC vs. Random Forest': svc_vs_forest,\n",
    "    'Logistic Regression vs. SVC': logreg_vs_svc,\n",
    "    'Logistic Regression vs. Random Forest': logreg_vs_forest,  \n",
    "    'All Models': all_model_compare,\n",
    "}\n",
    "\n",
    "row_names = ['Stat Value', 'P value', 'Comparison Result']\n",
    "pd.DataFrame(comparison_values, index=row_names).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-cooperation",
   "metadata": {},
   "source": [
    "**Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-fight",
   "metadata": {},
   "source": [
    "Selection of the model was chosen based on the performance of the model (given in the output following 10 k-fold validation, the previous results following ANOVA analysis between the 3 generated models and the standard deviation and mean values of the scores for each model. As stated previously, the SVC model was of the same distribution as both the random forest model and the logistic regression model. The random forest model however, was of a different distribution as the random forest model. Overall, all three models, when compared together, were found to be of different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "lucky-colonial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Logistic Regression Model</th>\n",
       "      <th>Best SVM Model</th>\n",
       "      <th>Best Random Forest Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Score Value</th>\n",
       "      <td>0.891905</td>\n",
       "      <td>0.934683</td>\n",
       "      <td>0.934524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard Deviation</th>\n",
       "      <td>0.050217</td>\n",
       "      <td>0.039762</td>\n",
       "      <td>0.031285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Best Logistic Regression Model  Best SVM Model  \\\n",
       "Mean Score Value                          0.891905        0.934683   \n",
       "Standard Deviation                        0.050217        0.039762   \n",
       "\n",
       "                    Best Random Forest Model  \n",
       "Mean Score Value                    0.934524  \n",
       "Standard Deviation                  0.031285  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We do not need to split the data into test and training set since this is just linear regression with a very small data set (41 observations)\n",
    "stats_scores = {\n",
    "    'Best Logistic Regression Model': logreg_stats_values['Scaled'],\n",
    "    'Best SVM Model': svm_stats_scores['Best RBF'],\n",
    "    'Best Random Forest Model': random_forest_mean_values['100 Estimators'],\n",
    "}\n",
    "\n",
    "pd.DataFrame(stats_scores, index=['Mean Score Value', 'Standard Deviation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-medicaid",
   "metadata": {},
   "source": [
    "For the selection of the best model, a high mean score value and a high standard deviation was desired. Therefore, the best model for the ionosphere data set is the SVM as it has a high mean score value  and the highest standard deviation (~0.04). Comparatively, the Random Forest Model had a comparable mean score value and a slightly lower standard deviation, meaning that the model could account for slightly less deviance from the mean value. The worst performing model of the 3 models was the logistic regression model; however, the model does account for the most variance across all 3 models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
